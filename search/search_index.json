{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction and Overview","text":"<p>FaaSr is serverless Function-as-a-Service cloud software that makes it easy for  developers to create functions and compose them in workflow graphs that can run  unattended and on-demand</p> <p>FaaSr simplifies deploying reproducible FaaS workflows in different cloud providers  without any code changes, with a primary use case in scientific workflows</p>"},{"location":"#cloud-platforms","title":"Cloud platforms","text":"<p>FaaSr supports the following cloud computing platforms:</p> <ul> <li>GitHub Actions - free-tier resources for small-scale workflows</li> <li>AWS Lambda - commercial cloud for scalable, low-latency worrkflows</li> <li>Google Cloud Run - commercial cloud for resource-intensive workflows</li> <li>OpenWhisk - open-source FaaS platform for private clouds</li> <li>Slurm - open-source job schedulers for private HPC clusters</li> </ul> <p>and the S3 protocol for cloud storage in platforms including:</p> <ul> <li>Minio - open-source S3 software</li> <li>AWS S3 - commercial S3 provider</li> <li>OSN - academic S3 provider in the USA</li> </ul>"},{"location":"#programming-languages","title":"Programming languages","text":"<p>FaaSr supports workflows with functions written in Python, R, or a combination of both</p>"},{"location":"#requirements","title":"Requirements","text":"<p>To get started with FaaSr you need:</p> <ul> <li>A GitHub account and associated credentials (a PAT token)</li> <li>An S3 data store bucket and associated credentials (access and secret keys)</li> <li>A FaaSr-workflow GitHub repository in your account</li> <li>A workflow configuration JSON file stored in your account's FaaSr-workflow repository</li> <li>A function code repository</li> </ul>"},{"location":"#getting-started","title":"Getting started","text":"<p>It's often easier to learn by doing: read the workflow model introduction for a brief overview of how FaaSr workflows are composed, and then the FaaSr tutorial guides you through setting up your ]workflow repo] using a freely available public S3 test bucket hosted by Minio Play. Once you are comfortable with the initial setup from the tutorial, you will be able to:</p> <ul> <li>Configure additional cloud compute and data storage accounts</li> <li>Create your own functions using FaaSr R APIs and/or FaaSr Python APIs</li> <li>Configure your own workflows using the FaaSr Workflow Builder Web UI that produces FaaSr-compliant JSON configurations</li> <li>Register your own workflows with one or more cloud providers</li> <li>Invoke your workflows</li> <li>Verify and debug your workflow logs</li> </ul>"},{"location":"about/","title":"About FaaSr","text":"<p>FaaSr is research software that evolved from the experience with FLARE to become a generally-applicable FaaS serverless middleware for R and Python functions</p> <p>The FaaSr software follows the MIT open-source license is funded in part by a grant from the National Science Foundation (OAC-2450241 and OAC-2311124)). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p> <p>You can help us show our impact to our sponsors and other users by linking to this Web site, and citing our software in publications that have resulted from the use of the software. If your project uses the software, please consider adding a \"Powered by FaaSr\" to your Web site, linking back to faasr.io</p> <p>If you publish a research paper that has leveraged the software, please include as references:</p> <ul> <li>'Sungjae Park, R. Quinn Thomas, Cayelan C. Carey, Austin D. Delany, Yun-Jung Ku, Mary E. Lofton, Renato J. Figueiredo, \u201cFaaSr: Cross-Platform Function-as-a-Service Serverless Scientific Workflows in R\u201d, 20th International IEEE eScience Conference, 2024'</li> <li>'Sungjae Park, Yun-Jung Ku, Nan Mu, Vahid Daneshmand, R. Quinn Thomas, Cayelan C. Carey, Renato J. Figueiredo, \u201cFaaSr: R Package for Function-as-a-Service Cloud Computing\u201d, Journal of Open Source Software, 9(103)'</li> </ul>"},{"location":"advanced/","title":"Advanced usage","text":""},{"location":"advanced/#creating-custom-container-images","title":"Creating custom container images","text":""},{"location":"advanced/#custom-compute-server-names-and-registerinvoke-actions","title":"Custom compute server names and register/invoke actions","text":""},{"location":"community/","title":"Community","text":"<p>The goal of FaaSr is to enable research and development in event-driven Function-as-a-Service workflows for science domains including (but not limited to) natural sciences. If you are a researcher interested in using the software for your project, we would love to hear from you - please contact the team and join the user's group to let us know more about how we can help, and if you'd like your project to be highlighted in this page.</p> <p>If you publish research results that use the software, please make sure you cite our software in your publications to help us show the impact of our software to our sponsors.</p>"},{"location":"community/#users-and-practitioners","title":"Users and practitioners","text":"<p>If you are a user, or potential user of the software, please join our user's group and connect with the team and other users.</p>"},{"location":"community/#code-developers","title":"Code developers","text":"<p>We are excited to bring additional developers to contribute code, as well as contributions in documentation, examples, vignettes, supporting additional FaaS platforms, and others.</p> <p>Our software repository is on GitHub, which many developers are familiar with. We follow a review process \u2013 pull requests are thoroughly reviewed by one or more experienced peer developers before being incorporated in the code base. Therefore, we expect code of good quality that is thoroughly tested before a pull request is accepted. We provide integration tests that allow developers to test their code before a pull request.</p> <p>You should feel free to fork our repositories and work on your own fork; if you would like to commit code to our branch, the actual steps of this process are outlined in a separate document. Contributors interested in embarking on implementing a project that may require significant changes to software modules are encouraged to contact us before doing so to make sure we understand the scope of the project.</p>"},{"location":"conditional/","title":"Conditional invocation","text":"<p>Conditional invocation</p>"},{"location":"contact/","title":"Contact","text":""},{"location":"contact/#connect-with-other-users","title":"Connect with other users","text":"<p>Join the FaaSr users' group to ask questions and connect with other users</p>"},{"location":"contact/#report-bugs-feature-requests","title":"Report bugs, feature requests","text":"<p>Submit an issue on GitHub if you would like to report a bug or feature request</p>"},{"location":"contact/#contact-project-leads","title":"Contact project leads","text":"<p>Send an email to the project development leads if you have other inquiries:</p> <ul> <li>Dr. Renato Figueiredo, Oregon State University</li> <li>Dr. R. Quinn Thomas, Virginia Tech</li> <li>Dr. Cayelan C. Carey, Virginia Tech</li> </ul>"},{"location":"credentials/","title":"Creating cloud credentials","text":""},{"location":"credentials/#s3-data-server","title":"S3 data server","text":"<ul> <li>In general, the credentials you need from your S3 account are the <code>AccessKey</code> and the <code>SecretKey</code>. These are akin to user names and passwords. </li> <li>How you obtain these credentials will depend on your provider (e.g. AWS S3, MINIO, OSN - see below)</li> <li>Paste the access key and the secret key as Repository secrets in your FaaSr-workflow as per the instructions in the workflow repo documentation</li> </ul>"},{"location":"credentials/#github-actions","title":"GitHub Actions","text":"<p>If you don't already have one, you need to generate a GitHub Personal Access Token (PAT) to configure FaaSr. Details on how to create a PAT are available here</p> <p>In summary: - In the upper-right corner of any page, click your profile photo, then click Settings. - In the left sidebar, click Developer settings. - In the left sidebar, click Personal access tokens. - Click Generate new token. - In the \"Note\" field, give your token a descriptive name. - In scopes, select \u201cworkflow\u201d and \u201cread:org\u201d (under admin:org) - Copy the token; you may save it in your local computer's password manager as well - Paste the token under the name <code>GH_PAT</code> as a Repository secret in your FaaSr-workflow as per the instructions in the workflow repo documentation</p>"},{"location":"credentials/#aws-lambda","title":"AWS Lambda","text":"<ul> <li>You need an access key and secret key to use Lambda in FaaSr.</li> <li>You can download your access and secret keys from your Amazon AWS console.</li> <li>Paste the access key and the secret key under the names <code>AWS_AccessKey</code> and <code>AWS_SecretKey</code>, respectively, as Repository secrets in your FaaSr-workflow as per the instructions in the workflow repo documentation</li> </ul>"},{"location":"credentials/#openwhisk","title":"OpenWHisk","text":"<ul> <li>You need an API key from your provider to configure for use in FaaSr</li> <li>How you obtain this will depend on your cloud provider.</li> <li>Paste the API key under the name <code>OW_APIkey</code> as a Repository secret in your FaaSr-workflow as per the instructions in the workflow repo documentation</li> </ul>"},{"location":"credentials/#google-cloud","title":"Google Cloud","text":"<p>TBD</p>"},{"location":"credentials/#slurm","title":"Slurm","text":"<p>TBD</p>"},{"location":"credentials/#academic-cloud-providers","title":"Academic cloud providers","text":""},{"location":"credentials/#open-storage-network","title":"Open Storage Network","text":"<ul> <li>For researchers in the US, you can request an allocation of 10+ TB S3 storage.</li> <li>If your request is approved, you will be assigned one S3 bucket, and can then copy the access and secret keys provided to you for use with FaaSr</li> </ul>"},{"location":"defaults/","title":"Default values","text":"<p>The following values are set by default in the Web UI and take effect unless overridden by the user:</p>"},{"location":"defaults/#compute-server-names","title":"Compute server names","text":"<ul> <li><code>GH</code>: GitHub Actions</li> <li><code>AWS</code>: AWS Lambda</li> <li><code>GCP</code>: Google Cloud</li> <li><code>OW</code>: OpenWhisk</li> <li><code>SLURM</code>: Slurm</li> </ul>"},{"location":"defaults/#compute-server-configurations","title":"Compute server configurations","text":""},{"location":"defaults/#githubactions","title":"GitHubActions","text":"<ul> <li><code>FaaSr-workflow</code>: workflow repository name</li> <li><code>main</code>: workflow repository branch</li> </ul>"},{"location":"defaults/#aws-lambda","title":"AWS Lambda","text":"<ul> <li><code>us-east-1</code>: region</li> </ul>"},{"location":"defaults/#openwhisk","title":"OpenWhisk","text":"<ul> <li><code>False</code>: allow self-signed certificates</li> </ul>"},{"location":"defaults/#data-store-configurations","title":"Data store configurations","text":"<ul> <li>'S3': default data store name</li> <li><code>True</code>: by default, an S3 store is set to writable</li> </ul>"},{"location":"defaults/#secrets","title":"Secrets","text":"<ul> <li><code>GH_PAT</code>: GitHub personal access token</li> <li><code>AWS_AccessKey</code>: AWS Lambda access key</li> <li><code>AWS_SecretKey</code>: AWS Lambda secret key</li> <li><code>GCP_SecretKey</code>: GCP secret key</li> <li><code>OW_APIkey</code>: OpenWhisk API key</li> <li><code>SLURM_Token</code>: SLURM JWT token</li> </ul>"},{"location":"defaults/#log-folder-in-s3","title":"Log folder (in S3)","text":"<ul> <li><code>FaaSrLog</code>: top-level log folder name; see log documentation for full path information</li> </ul>"},{"location":"defaults/#containers","title":"Containers","text":"<p>Note: the latest stable release is tagged as <code>:latest</code> in all containers; specific release versions are tagged with the version number, e.g. <code>:2.0.0</code> for version 2.0.0</p>"},{"location":"defaults/#python","title":"Python","text":"<ul> <li><code>ghcr.io/faasr/github-actions-python:latest</code>: GitHub Actions (in GHRC)</li> <li><code>145342739029.dkr.ecr.us-east-1.amazonaws.com/aws-lambda-python:latest</code>: AWS Lambda east-1 (in ECR)</li> <li><code>faasr/gcp-python:latest</code>: GCP (in DockerHub)</li> <li><code>faasr/openwhisk-python:latest</code>: OpenWhisk (in DockerHub)</li> <li><code>faasr/slurm-python:latest</code>: Slurm (in DockerHub)</li> </ul>"},{"location":"defaults/#r","title":"R","text":"<ul> <li><code>ghcr.io/faasr/github-actions-r:latest</code>: GitHub Actions (in GHRC)</li> <li><code>145342739029.dkr.ecr.us-east-1.amazonaws.com/aws-lambda-r:latest</code>: AWS Lambda east-1 (in ECR)</li> <li><code>faasr/gcp-r:latest</code>: GCP (in DockerHub)</li> <li><code>faasr/openwhisk-r:latest</code>: OpenWhisk (in DockerHub)</li> <li><code>faasr/slurm-r:latest</code>: Slurm (in DockerHub)</li> </ul>"},{"location":"functions/","title":"Creating functions","text":""},{"location":"functions/#overview","title":"Overview","text":"<p>Creating a function for use in FaaSr entails the following steps:</p> <ul> <li>Select a GitHub repository to store your function code; we'll use MyGitHubAccount and MyFunctionRepo as a GitHub account name and repository name, respectively, in examples</li> <li>Develop the code for your function. A best practice is to have one file per function; we will use <code>compute_sum.R</code> and <code>compute_sum.py</code> as examples</li> <li>Add FaaSr API calls where appropriate, e.g. <code>faasr_get_file()</code> to get an input file from an S3 data server, <code>faasr_put_file()</code> to put an output file to an S3 data server, <code>faasr_log()</code> to write a message to the log. Refer to the FaaSr R APIs and FaaSr Python APIs documents for a complete list</li> <li>Add the function to a workflow. This is done using the FaaSr Workflow Builder Web UI by clicking on an Action in the workflow DAG</li> </ul>"},{"location":"functions/#example","title":"Example","text":"<p>Let's say you develop a function <code>compute_sum.R</code> (e.g. the one used in the FaaSr tutorial) as follows:</p> <pre><code>compute_sum &lt;- function(folder, input1, input2, output) {\n\n  # FaaSr API calls to get inputs from S3 (two CSV files)\n  faasr_get_file(remote_folder=folder, remote_file=input1, local_file=\"input1.csv\")\n  faasr_get_file(remote_folder=folder, remote_file=input2, local_file=\"input2.csv\")\n\n  # Function's main implementation (compute a sum and write the output)\n  frame_input1 &lt;- read.table(\"input1.csv\", sep=\",\", header=T)\n  frame_input2 &lt;- read.table(\"input2.csv\", sep=\",\", header=T)\n  frame_output &lt;- frame_input1 + frame_input2\n  write.table(frame_output, file=\"output.csv\", sep=\",\", row.names=F, col.names=T)\n\n  # FaaSr API call to put the output file in the S3 bucket\n  faasr_put_file(local_file=\"output.csv\", remote_folder=folder, remote_file=output)\n\n  # Log a message\n  log_msg &lt;- paste0('Function compute_sum finished; output written to ', folder, '/', output, ' in default S3 bucket')\n  faasr_log(log_msg)\n}   \n</code></pre> <p>Say you commit <code>compute_sum.R</code> to repository <code>MyGitHubAccount/MyFunctionRepo</code>. </p> <p>To use this function in a workflow, in the FaaSr Workflow Builder Web UI proceed as follows:</p> <p></p> <ul> <li>Create an Action (e.g. compute_sum)</li> <li>Select it to edit using the left pane</li> <li>Under Function Name, enter <code>compute_sum</code>; this is the name of the function declared in the code above</li> <li>Under Compute Server, select your compute server from the drop-down menu (e.g. GH for GitHub Actions)</li> <li>Under Arguments, enter names and values of the arguments matching those used by the function: <code>folder</code>, <code>input1</code>, <code>input2</code>, <code>output</code></li> <li>Under Function's Git Repo/Path, enter <code>MyGitHubAccount/MyFunctionRepo</code></li> </ul>"},{"location":"functions/#important-notes","title":"Important notes","text":"<ul> <li>If you provide a GitHub repo (e.g. <code>MyGitHubAccount/MyFunctionRepo</code>) Function's Git Repo/Path, FaaSr will clone and source all source code files from it; e.g. if your repository has files <code>compute_sum.R</code>, <code>compute_mult.R</code>, etc, each will be sourced</li> <li>You can, alternatively, provide a path to a file in a GitHub repo, e.g. <code>MyGitHubAccount/MyFunctionRepo/compute_sum.R</code>; this will only fetch and source one function</li> </ul>"},{"location":"invocationid/","title":"Invocation IDs","text":"<p>Purpose of IDs: - Unique location for logs - Deriving unique file/folder names</p>"},{"location":"invocationid/#default-uuid","title":"Default - UUID","text":""},{"location":"invocationid/#custom-user-provided-id","title":"Custom: User-provided ID","text":""},{"location":"invocationid/#timestamp","title":"Timestamp","text":""},{"location":"invoke_workflow/","title":"Invoke workflows","text":"<p>Invoke workflows</p>"},{"location":"logs/","title":"Logs","text":""},{"location":"logs/#faasr-logs","title":"FaaSr logs","text":"<p>Logs produced by FaaSr functions using the <code>faasr_log()</code> API are stored in an S3 server. This document overviews how to select an S3 server to store these logs, and the folder naming structure to help you locate a particular log</p>"},{"location":"logs/#selecting-an-s3-bucket-for-logs","title":"Selecting an S3 bucket for logs","text":"<ul> <li>In the typical scenario, a single S3 server/bucket can be used for both workflow files and logs, but you may also use separate buckets</li> <li>The S3 server used for logs is configured as the Default server to store logs under Workflow settings in the FaaSr Workflow Builder Web UI </li> </ul>"},{"location":"logs/#log-folder-naming-convention","title":"Log folder naming convention","text":"<ul> <li>Using the FaaSr Workflow Builder Web UI you must name your workflow uniquely; we'll use <code>WorkflowName</code> in this example</li> <li>Using the FaaSr Workflow Builder Web UI you can also provide a name for the top-level log folder; if you do not provide one, by default the name is is set to <code>FaaSrLog</code></li> <li>Then, the path where you can find logs for a particular Action function execution is:</li> </ul> <p><code>FaaSrLog/WorkflowName/InvocationTimestamp/InvocationID/ActionName.txt</code></p> <p>Where: - <code>ActionName</code> is the name of the action (i.e. the name of a node in your DAG) - <code>InvocationID</code> is a specific invocation of your Action. Refer to the invocationID documentation for the different types of invocation IDs supported (UUID, user-provided, and timestamp-derived) - <code>InvocationTimestamp</code> is a string-formatted invocation timestamp recorded at the time your workflow's entry action starts execution. It is recorded as an ISO 8601-like timestamp with filename-safe separators</p>"},{"location":"logs/#examples","title":"Examples","text":"<ul> <li>Suppose you run the FaaSr tutorial as an example</li> <li>In this workflow configuration, the log folder is the default <code>FaaSrLog</code> and the workflow name is <code>FaaSrTutorial</code></li> <li>Suppose you also leave the <code>InvocationID</code> as default, which generates a unique UUID automatically at the start entry point. In this example, let's assume the UUID generated is: <code>f81d4fae-7dec-11d0-a765-00a0c91e6bf6</code></li> <li>Suppose when you invoke the workflow, the current time seen by the start action was timestamped as <code>2025-08-29T23-17-01</code></li> </ul> <p>You will then be able to retrieve logs for the compute_sum action from the (unique) path: </p> <p><code>FaaSrLog/FaaSrTutorial/2025-08-29T23-17-01/f81d4fae-7dec-11d0-a765-00a0c91e6bf6/compute_sum.txt</code></p> <p>If you invoke the workflow again, a new timestamp and UUID will be generated</p>"},{"location":"logs/#cloud-logs","title":"Cloud logs","text":"<ul> <li>In addition to FaaSr-specific logs (i.e. where you use the <code>faasr_log()</code> API), you will find provider-specific action logs in your cloud provider of choice</li> <li>Please refer to their documentation:</li> <li>GitHub Action logs</li> <li>AWS Lambda logs</li> <li>Google cloud logs</li> </ul>"},{"location":"prog_model/","title":"FaaSr workflow model","text":"<p>The FaaSr model for programming, configuring, and deploying workflows is as follows:</p> <ul> <li>FaaSr Workflow DAG: describes the order in which your functions execute. Valid workflows must be Directed Acyclic Graphs (DAG) (i.e. no loops) with a single invocation entry, and are described as a JSON file</li> <li>Action: each node in the DAG is an Action that represents the execution of a user function in a container deployed at runtime in a serverless cloud provider (e.g. a GitHub Action, or AWS Lambda)</li> <li>Function: once deployed, each Action executes a user-defined Function in the contaier. A Function is written in a high level language and published as a .R or .py file in a GitHub Function repo. A Function takes input arguments and returns either true (successful excecution) or false (unsuccessful execution)</li> <li>Input/output: a serverless Action is stateless: the local memory and storage in a container is discarded when the Action completes. Therefore, all data that must persist across invocations must be stored as files in one or more S3 cloud data store servers. FaaSr provides an Application Programming Interface (API) to store/retrieve files to/from S3 for this.</li> </ul>"},{"location":"prog_model/#workflow-dag-example","title":"Workflow DAG example","text":"<p>To illustrate how a FaaSr workflow DAG works, consider the example below:</p> <p></p> <ul> <li>This DAG has seven Actions, where the invocation entry is a node named start</li> <li>Action start has two successor actions: computeA and computeB; this means that when start finishes executing, both computeA and computeB are invoked, concurrently</li> <li>Action computeA invokes ten instances of action concurrent; each of these actions is provided with a unique Rank (in this example, a number from 1 to 10)</li> <li>Action computeB invokes either conditionalT or conditionalF, depending on whether computeB returns True or False, respectively</li> <li>Finally, action end only executes after all its predecessors in the DAG finish their execution - i.e. after all 10 instances of concurrent and either conditionalT or conditionalF finish</li> <li>In this DAG, the concurrent actions execute as AWS Lambdas, while all other actions execute as GitHub Actions</li> </ul>"},{"location":"prog_model/#actions-functions-and-inputoutput-files","title":"Actions, functions, and input/output files","text":"<p>The example workflow DAG above names which actions should execute, and in which order. Each action is essentially a container (e.g. a Docker instance) that is deployed on a serverless cloud provider - which means these actions are invoked, execute for some time, then are terminated. Let's dive deeper now into: how to define which Function each action executes? How to save and retrieve inputs and outputs?</p> <p></p> <ul> <li>Action computeA runs a function written in R, while concurrent runs a function written in Python. These stored in (one or more) GitHub function repo(s) and are fetched automatically by the action when it is invoked</li> <li>Actions can read input files and write output files to (one or more) S3 data store(s). the FaaSr API provides functions in R and Python to put files to/get files from data server(s). The FaaSr API simplifies the programming of functions to use data from S3, without exposing you (the programmer or user) to details on how to access S3</li> <li>In general, the bulk of the data in a workflow consists of files; function arguments are used for configuration (e.g. of parameters, file and folder names)</li> <li>The typical file access pattern is as follows: 1) an action starts; 2) the action gets input file(s), copying them from persistent S3 storage into its local temporary storage; 3) the action produces output file(s) in its local non-persistent storage; 4) the action puts its output file(s) to persistent S3 storage; and 5) the action ends </li> </ul>"},{"location":"prog_model/#creating-editing-registering-and-invoking-workflows","title":"Creating, editing, registering and invoking workflows","text":"<p>We've seen what the workflow DAG represents (actions and their orders) and we also have seen that workflow DAGs themselves are stored in files in the JSON format. Putting it all together, we need ways to: 1) create and edit workflow JSON files using the FaaSr workflow builder Web graphical user interface; 2) store these workflow JSON configuration files in a FaaSr-workflow repository on GitHub; and 3) register and invoke these workflows in your cloud provider(s) of choice. Consider the example below that illustrates how this process typically works:</p> <p></p> <ul> <li>The user first needs to fork the FaaSr-workflow repo from the FaaSr organization. This will be your main environment for managing all your workflows (you can also have multiple workflow repos, though typically a single repo will suffice)</li> <li>Let's assume the user already has one or more workflows in this repo (e.g. the tutorial workflow). Then the user can use the FaaSr workflow builder to 1) import the workflow JSON file from their FaaSr-workflow repo; 2) edit with the Web UI; 3) download the JSON file to their computer; 4) commit the JSON file to their workflow repo; 5) register the workflow with their cloud provider(s); and 6) invoke the workflow so it executes in the cloud</li> <li>Once a workflow is registered, it can be invoked multiple times, either as manual, user-initiated one-shot invocations, or as an automatic, unattended periodic timer invocation</li> <li>The registration and invocation of workflows are themselves implemented as GitHub Actions inherited from the forked FaaSr-workflow repo</li> <li>The FaaSr-workflow repo also stores Secrets which are the credentials needed to access your cloud provider(s) of choice</li> </ul>"},{"location":"prog_model/#container-images-and-package-dependences","title":"Container images and package dependences","text":"<ul> <li>FaaSr provides a set of base images for containers with runtime environments for both Python and R, and deployable on the supported cloud providers. These images are typically sufficient for most use cases.</li> <li>If a function needs additional packages (e.g. from CRAN, PyPI), those can be declared in the workflow JSON configuration and fetched automatically.</li> <li>If a function needs a custom container image, the user can create their own custom container image(s), copy them to the appropriate container registry (e.g. DockerHub, GHCR, ECR), and also declare them in the workflow JSON configuration</li> </ul>"},{"location":"py_api/","title":"Python APIs","text":"<p>Python APIs</p>"},{"location":"r_api/","title":"R APIs","text":""},{"location":"r_api/#faasr_get_file","title":"faasr_get_file","text":"<p>Usage: <code>faasr_get_file(server_name, remote_folder, remote_file, local_folder, local_file)</code></p> <p>This function gets (i.e. downloads) a file from an S3 bucket to be used by the FaaSr function.</p> <p><code>server_name</code> is a string with name of the S3 bucket to use; it must match a name declared in the workflow configuration JSON file. This is an optional argument; if not provided, the default S3 server specified as <code>DefaultDataStore</code> in the workflow configuration JSON file is used.</p> <p><code>remote_folder</code> is string with the name of the remote folder where the file is to be downloaded from. This is an optional argument that defaults to <code>\"\"</code></p> <p><code>remote_file</code> is a string with the name for the file to be downloaded from the S3 bucket. This is a required argument.</p> <p><code>local_folder</code> is a string with the name of the local folder where the file to be downloaded is stored. This is an optional argument that defaults to <code>\".\"</code></p> <p><code>local_file</code> is a string with the name for the file downloaded from the S3 bucket. This is a required argument.</p> <p>Examples:</p> <pre><code>faasr_get_file(remote_folder=\"myfolder\", remote_file=\"myinput1.csv\", local_file=\"input1.csv\")\nfaasr_get_file(server_name=\"My_Minio_Bucket\", remote_file=\"myinput2.csv\", local_file=\"input2.csv\")\n</code></pre>"},{"location":"r_api/#faasr_put_file","title":"faasr_put_file","text":"<p>Usage: <code>faasr_put_file(server_name, local_folder, local_file, remote_folder, remote_file)</code></p> <p>This function puts (i.e. uploads) a file from the local FaaSr function to an S3 bucket.</p> <p><code>server_name</code> is a string with name of the S3 bucket to use; it must match a name declared in the workflow configuration JSON file. This is an optional argument; if not provided, the default S3 server specified as <code>DefaultDataStore</code> in the workflow configuration JSON file is used.</p> <p><code>local_folder</code> is a string with the name of the local folder where the file to be uploaded is stored. This is an optional argument that defaults to <code>\".\"</code></p> <p><code>local_file</code> is a string with the name for the file to be uploaded to the S3 bucket. This is a required argument.</p> <p><code>remote_folder</code> is string with the name of the remote folder where the file is to be uploaded to. This is an optional argument that defaults to <code>\"\"</code></p> <p><code>remote_file</code> is a string with the name for the file to be uploaded to the S3 bucket. This is a required argument.</p> <p>Examples:</p> <pre><code>faasr_put_file(local_file=\"output.csv\", remote_folder=\"myfolder\", remote_file=\"myoutput.csv\")\nfaasr_get_file(server_name=\"My_Minio_Bucket\", local_file=\"output.csv\", remote_file=\"myoutput.csv\")\n</code></pre>"},{"location":"r_api/#faasr_get_folder_list","title":"faasr_get_folder_list","text":"<p>Usage: <code>folderlist &lt;- faasr_get_folder_list(server_name, faasr_prefix)</code></p> <p>This function returns a list with the contents of a folder in the S3 bucket. </p> <p><code>server_name</code> is a string with name of the S3 bucket to use; it must match a name declared in the workflow configuration JSON file. This is an optional argument; if not provided, the default S3 server specified as <code>DefaultDataStore</code> in the workflow configuration JSON file is used.</p> <p><code>faasr_prefix</code> is a string with the prefix of the folder in the S3 bucket. This is an optional argument that defaults to <code>\"\"</code></p> <p>Examples:</p> <pre><code>mylist1 &lt;- faasr_get_folder_list(server_name=\"My_Minio_Bucket\", faasr_prefix=\"myfolder\")\nmylist2 &lt;- faasr_get_folder_list(server_name=\"My_Minio_Bucket\", faasr_prefix=\"myfolder/mysubfolder\")\n</code></pre>"},{"location":"r_api/#faasr_delete_file","title":"faasr_delete_file","text":"<p>Usage: <code>faasr_delete_file(server_name, remote_folder, remote_file)</code></p> <p>This function deletes a file from the S3 bucket.</p> <p><code>server_name</code> is a string with name of the S3 bucket to use; it must match a name declared in the workflow configuration JSON file. This is an optional argument; if not provided, the default S3 server specified as <code>DefaultDataStore</code> in the workflow configuration JSON file is used.</p> <p><code>remote_folder</code> is string with the name of the remote folder where the file is to be deleted from. This is an optional argument that defaults to <code>\"\"</code></p> <p><code>remote_file</code> is a string with the name for the file to be deleted from the S3 bucket. This is a required argument.</p> <p>Examples:</p> <pre><code>faasr_delete_file(remote_folder=\"myfolder\", remote_file=\"myoutput.csv\")\nfaasr_delete_file(server_name=\"My_Minio_Bucket\", remote_file=\"myoutput.csv\")\n</code></pre>"},{"location":"r_api/#faasr_arrow_s3_bucket","title":"faasr_arrow_s3_bucket","text":"<p>Usage: <code>faasr_arrow_s3_bucket(server_name, faasr_prefix)</code></p> <p>This function configures an S3 bucket to use with Apache Arrow.</p> <p><code>server_name</code> is a string with name of the S3 bucket to use; it must match a name declared in the workflow configuration JSON file. This is an optional argument; if not provided, the default S3 server specified as <code>DefaultDataStore</code> in the workflow configuration JSON file is used.</p> <p><code>faasr_prefix</code> is a string with the prefix of the folder in the S3 bucket. This is an optional argument that defaults to <code>\"\"</code></p> <p>It returns a list that is subsequently used with the Arrow package.</p> <p>Examples:</p> <pre><code>mys3 &lt;- faasr_arrow_s3_bucket()\nmyothers3 &lt;- faasr_arrow_s3_bucket(server_name=\"My_Minio_Bucket\", faasr_prefix=\"myfolder\")\nframe_input1 &lt;- arrow::read_csv_arrow(mys3$path(file.path(folder, input1)))\nframe_input2 &lt;- arrow::read_csv_arrow(mys3$path(file.path(folder, input2)))\narrow::write_csv_arrow(frame_output, mys3$path(file.path(folder, output)))\n</code></pre>"},{"location":"r_api/#faasr_log","title":"faasr_log","text":"<p>Usage: <code>faasr_log(log_message)</code></p> <p>This function writes a log message to a file in the S3 bucket, to help with debugging. The default S3 server for logs is <code>DefaultDataStore</code> as specified in the workflow configuration JSON file. This default can be overridden with <code>LoggingDataStore</code> in  the workflow configuration JSON file.</p> <p><code>log_message</code> is a string with the message to be logged.</p> <p>Example:</p> <pre><code>log_msg &lt;- paste0('Function compute_sum finished; output written to ', folder, '/', output, ' in default S3 bucket')\nfaasr_log(log_msg)\n</code></pre>"},{"location":"r_api/#faasr_rank","title":"faasr_rank","text":"<p>Usage: <code>faasr_rank()</code></p> <p>Only applicable for workflows with functions that are triggered for concurrent execution using rank. This returns this function's invocation rank as a list with two values:</p> <p><code>list$MaxRank</code> - an integer that determines to the maximum number of invocations (e.g. N) <code>list$Rank</code> - an integer that determines this function's invocation rank (e.g. a number ranging from 1 to N)</p> <p>Example:</p> <p>In a workflow, suppose you have declared that a function <code>start</code> invokes a successor function <code>compute</code> with rank N=10, where each instance of <code>compute</code> works to produce an output file <code>myoutput_i.csv</code>, where i is a number from 1 to 10.</p> <p>At runtime, <code>start</code> invokes 10 instances of <code>compute</code>; <code>faasr_rank()</code> determines the value of <code>i</code> for each instance of <code>compute</code>, which can be used to construct the file name:</p> <pre><code>rank_list &lt;- faasr_rank()\nmy_rank &lt;- rank_list$Rank\nmax_rank &lt;- rank_list$MaxRank\nlocal_file &lt;- paste0(\"myinput_\", my_rank, \".csv\")\nwrite.csv(my_data, local_file, row.names=FALSE)\n\n</code></pre>"},{"location":"r_api/#faasr_invocation_id","title":"faasr_invocation_id","text":"<p>Usage: <code>faasr_invocation_id()</code></p> <p>Returns this action's invocation ID.</p> <p>Example:</p> <pre><code>invocation_id &lt;- faasr_invocation_id()\n</code></pre>"},{"location":"rank/","title":"Concurrent invocation","text":"<p>Concurrent invocation</p>"},{"location":"register_workflow/","title":"Register workflows","text":"<p>Register workflows</p>"},{"location":"tutorial/","title":"Tutorial","text":"<p>This document guides you through a simple tutorial that uses GitHub Actions and a free S3 data store (Minio Play). The pre-requisite for this tutorial is a GitHub account.</p>"},{"location":"tutorial/#configure-your-faasr-workflow-repository","title":"Configure your FaaSr-workflow repository","text":"<p>This is a one-time step you go through to set up your workflow repository to host all your workflows, including the tutorial.</p> <ul> <li>Navigate to the FaaSr organization workflow repo</li> <li>Fork the repository to your own GitHub account, keeping the FaaSr-workflow name</li> <li>In your forked repository, click on Actions and click on I understand my workflows, go ahead and enable them to allow FaaSr register/invoke actions to execute</li> </ul>"},{"location":"tutorial/#configure-your-github-pat","title":"Configure your GitHub PAT","text":"<p>You need a personal access token in your repository secrets to run this tutorial. </p> <p>Follow the steps outlined in the credentials documentation to obtain your PAT. Copy this PAT so you can paste it as a secret in the next step</p>"},{"location":"tutorial/#configure-your-repository-secrets","title":"Configure your repository secrets","text":"<p>Before you can register and invoke workflows, you need to create secrets storing credentials for the cloud providers you will use. The following assumes that you already have obtained [cloud credentials] for those.</p> <ul> <li>In the FaaSr-workflow repo you just forked, click on the Settings tab (top of the page, to the right)</li> <li>Scroll down; on the left pane, click on the pull-down Secrets and variables and select Actions</li> <li>Click on the green New repository secret to enter a new secret</li> <li>Enter the proper Name for each of the three secrets below (one for GitHub actions, two for Minio Play) and past the secret itself in the Secret text box:</li> <li>Click on Add secret and add a secret named <code>GH_PAT</code>, pasting your GitHub PAT created in the previous step</li> <li>Click on Add secret and add a secret named <code>S3_ACCESSKEY</code>, pasting the following text: <code>Q3AM3UQ867SPQQA43P2F</code></li> <li>Click on Add secret and add a secret named <code>S3_SECRETKEY</code>, pasting the following text: <code>zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG</code></li> </ul>"},{"location":"tutorial/#edit-the-tutorial-json-file","title":"Edit the tutorial JSON file","text":"<p>The tutorial.json file comes by default in the repository you forked; however, you still need to configure it with your GitHub account name. This can be done in either of two ways:</p>"},{"location":"tutorial/#option-1-edit-tutorialjson-in-github","title":"Option 1: Edit tutorial.json in GitHub","text":"<ul> <li>Edit the tutorial, json file in GitHub</li> <li>Search for myusername, and replace that with your GitHub account name</li> <li>Commit the file</li> </ul>"},{"location":"tutorial/#option-2-edit-tutorialjson-in-the-web-ui","title":"Option 2: Edit tutorial.json in the Web UI","text":"<ul> <li>Browse to the FaaSr Workflow Builder Web UI</li> <li>Click on Upload (top left)</li> <li>To import the tutorial file from GitHub, enter in text box: <code>https://github.com/FaaSr/FaaSr-workflow/blob/main/tutorial.json</code></li> <li>Click on Edit Compute Servers, click on GH, replace the UserName with your GitHub account name</li> <li>Download tutorial.json to your computer</li> <li>Upload tutorial.json to your repository (replacing the original file)</li> </ul>"},{"location":"tutorial/#register-the-workflow","title":"Register the workflow","text":"<ul> <li>You first need to register the workflow before it can be invoked</li> <li>Click on Actions</li> <li>Click on <code>(FAASR REGISTER)</code> (left)</li> <li>Click on the <code>Run workflow</code> drop-down; enter <code>tutorial.json</code> and click on <code>Run workflow</code></li> <li>This will take a few seconds to complete; wait until the action completes before proceeding</li> </ul>"},{"location":"tutorial/#invoke-the-workflow","title":"Invoke the workflow","text":"<ul> <li>After register, you can invoke the workflow </li> <li>Click on Actions</li> <li>Click on <code>(FAASR INVOKE)</code> (left)</li> <li>Click on the <code>Run workflow</code> drop-down; enter <code>tutorial.json</code> and click on <code>Run workflow</code></li> <li>This will take a few seconds to complete; wait until the action completes before proceeding</li> </ul>"},{"location":"tutorial/#check-outputs","title":"Check outputs","text":"<p>The workflow outputs are stored in the Minio Play S3 bucket. There are different ways you can access it, with different S3 clients (e.g. Minio client); we will use the Minio Play console in this tutorial:</p> <ul> <li>Browse to minio play web UI</li> <li>For user name, enter <code>Q3AM3UQ867SPQQA43P2F</code></li> <li>For password, enter <code>zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG</code></li> <li>Browse to faasr/tutorial to see the output files</li> </ul>"},{"location":"tutorial/#another-example","title":"Another example","text":"<p>The tutorial.json workflow is based on two R functions (start, compute_sum). Another example is found in tutorialRpy.json that uses an R function (start) followed by a Python function (compute_sum). If you'd like to test this out, repeat the edit, register, and invoke workflow steps as above for this workflow </p>"},{"location":"workflow_repo/","title":"Configuring your FaaSr-workflow repo","text":"<p>As described in the workflow model document, before you start using FaaSr to run workflows, you need to setup your FaaSr-workflow GitHub repository to hold the following information:</p> <ul> <li>JSON configuration files for each of your workflows</li> <li>GitHub secrets for your compute and data cloud servers</li> <li>GitHub actions that allow you to register and invoke workflows</li> </ul> <p>The simplest way to configure this repository is to fork from the FaaSr organization's FaaSr-workflow repository</p>"},{"location":"workflow_repo/#forking-the-base-repository","title":"Forking the base repository","text":"<ul> <li>In your browser, navigate to the FaaSr-workflow base repository</li> <li>Click on the down arrow next to Fork for a pull-down menu; select Create a new fork</li> <li>Choose your account name as the Owner of the fork</li> <li>While you can choose a different name for your fork, here we assume you leave the default FaaSr-workflow</li> <li>Click on the green Create fork button</li> </ul> <p>The base repository comes with an example workflow used in the FaaSr tutorial. </p>"},{"location":"workflow_repo/#enabling-faasr-actions","title":"Enabling FaaSr actions","text":"<p>In order to use register and invoke workflows, you also need to perform a one-time configuration to enable running the pre-defined FaaSr register and invoke workflow actions. To do this:</p> <ul> <li>Click on the Actions tab (top of the GitHub page, next to Code and Pull requests)</li> <li>Click on the green button I understand my workflows, please go ahead and enable them</li> </ul>"},{"location":"workflow_repo/#configuring-secrets","title":"Configuring secrets","text":"<p>Before you can register and invoke workflows, you need to create secrets storing credentials for the cloud providers you will use. The following assumes that you already have obtained cloud credentials for those.</p> <ul> <li>In the FaaSr-workflow repo you just forked, click on the Settings tab (top of the page, to the right)</li> <li>Scroll down; on the left pane, click on the pull-down Secrets and variables and select Actions</li> <li>Click on the green New repository secret to enter a new secret</li> <li>Enter the proper Name for your secret (see below) and past the secret itself in the Secret text box</li> <li>Click on Add secret</li> </ul>"},{"location":"workflow_repo/#naming-convention-for-secrets","title":"Naming convention for secrets","text":""},{"location":"workflow_repo/#s3-data-store-servers","title":"S3 data store servers","text":"<ul> <li>When creating a workflow with the FaaSr Workflow Builder Web UI, you are asked to enter a name for your S3 data server(s)</li> <li>The default compute server name for an S3 server is <code>S3</code></li> <li>Assume the name of a data server you are setting the secrets for is <code>S3</code>, you need two secrets, named exactly as follows (replace <code>S3</code> with the name of the server you configured)</li> <li><code>S3_AccessKey</code></li> <li><code>S3_SecretKey</code></li> <li>The secrets you store under these names are the access and secret keys you obtained from your S3 provider</li> </ul>"},{"location":"workflow_repo/#github-actions","title":"GitHub Actions","text":"<ul> <li>The default compute server name for GitHub Actions is <code>GH</code></li> <li>You should not modify this name unless you plan to use FaaSr with custom advanced actions</li> <li>You need one secret, named <code>GH_PAT</code></li> <li>The secret you store under this name is a GitHub Personal Access Token</li> </ul>"},{"location":"workflow_repo/#aws-lambda","title":"AWS Lambda","text":"<ul> <li>The default compute server name for AWS Lambda is <code>AWS</code></li> <li>You should not modify this name unless you plan to use FaaSr with custom advanced actions</li> <li>You need two secrets, named: <code>AWS_AccessKey</code> and <code>AWS_SecretKey</code></li> <li>The secrets you store under these names are the access and secret keys you obtained from AWS Lambda</li> </ul>"},{"location":"workflow_repo/#google-cloud","title":"Google Cloud","text":"<ul> <li>The default compute server name for AWS Lambda is <code>GCP</code></li> <li>You should not modify this name unless you plan to use FaaSr with custom advanced actions</li> <li>You need one secret named: <code>GCP_SecretKey</code> </li> <li>The secret you store under this name is the secret key you obtained from Google Cloud</li> </ul>"},{"location":"workflow_repo/#openwhisk","title":"OpenWhisk","text":"<ul> <li>The default compute server name for AWS Lambda is <code>OW</code></li> <li>You should not modify this name unless you plan to use FaaSr with custom advanced actions</li> <li>You need one secret named: <code>OW_API.key</code> </li> <li>The secret you store under this name is the API key you obtained from your OpenWhisk provider</li> </ul>"},{"location":"workflow_repo/#slurm","title":"Slurm","text":"<ul> <li>The default compute server name for AWS Lambda is <code>SLURM</code></li> <li>You should not modify this name unless you plan to use FaaSr with custom advanced actions</li> <li>You need one secret named: <code>SLURM_Token</code> </li> <li>The secret you store under this name is the JWT token you obtained from your Slurm provider</li> </ul>"},{"location":"workflows/","title":"Workflows","text":"<p>Workflows</p>"}]}